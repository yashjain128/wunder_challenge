# Welcome to the Wunder Challenge 2!
2025-12-30

We're excited to have you here. This is a machine learning competition where you'll build a model to predict future price movements based on Limit Order Book (LOB) data and recent trades. Itâ€™s a challenging sequence modeling problem from the domain of high-frequency trading. Let's get started!

## Your mission

Your goal is to predict two target variables (**t0** and **t1**) which represent two different types of future price movements. You will make these predictions based on the sequence of market states (LOB and trades) that came before. You'll be given the market's history up to a certain point, and you need to forecast these future indicators.

## How it works

The dataset is a single table in Parquet format, containing multiple independent sequences. Hereâ€™s what you need to know.

### The data format

Each row in the table represents a single market state at a specific step in a sequence. The table has the following columns:

*   `seq_ix`: An ID for the sequence. When this number changes, you're starting a new, completely independent sequence.
*   `step_in_seq`: The step number within a sequence (from 0 to 999).
*   `need_prediction`: A boolean thatâ€™s `True` if we need a prediction from you for this step, and `False` otherwise.

**Feature Columns:**
The numeric features provide a structured representation of the market state, derived from the Limit Order Book (LOB) and recent trade activity. While the data is anonymized, the structure follows standard market data conventions:

*   `p0`...`p5`: **Bid Price Features**. Anonymized features derived from the bid side of the Limit Order Book representing prices at different levels.
*   `p6`...`p11`: **Ask Price Features**. Anonymized features derived from the ask side of the Limit Order Book representing prices at different levels.
*   `v0`...`v5`: **Bid Volume Features**. Anonymized features representing volumes corresponding to price levels on the bid side.
*   `v6`...`v11`: **Ask Volume Features**. Anonymized features representing volumes corresponding to price levels on the ask side.
*   `dp0`...`dp3`: **Trade Price Features**. Anonymized features representing trade prices (derived from both bid and ask trades).
*   `dv0`...`dv3`: **Trade Volume Features**. Anonymized features representing trade volumes (derived from both bid and ask trades).

**Target Columns:**
*   `t0`, `t1`: The targets represent two different types of future price movement indicators derived from the market state. These values are also anonymized. Your task is to predict these values.

### The sequences

Each sequence is exactly **1000 steps** long.

> **Note:**
> The first 99 steps (0-98) of every sequence are for warm-up. Your model can use them to build context, but we won't score your predictions here. Your score comes from predictions for the remaining 901 steps (steps 99 to 999).

Because each sequence is independent, you must reset your modelâ€™s internal state whenever you see a new `seq_ix`.

You can also rely on two key facts about the data ordering:
*   **Within a sequence**, all steps are ordered by time.
*   **The sequences themselves** are randomly shuffled, so `seq_ix` and `seq_ix + 1` are not related.

> **Tip: How to create a validation set**
> We provide a dedicated `valid.parquet` file which you can use to test your model's performance. However, since all sequences are independent, you are not limited to this split. You can combine the datasets and create your own custom cross-validation strategy (e.g., K-Fold split by `seq_ix`) to potentially get more robust estimates.

## Evaluation and metrics

We'll evaluate your predictions using the **Weighted Pearson Correlation Coefficient** score.

The metric emphasizes performance on data points with larger target amplitudes (larger price movements). Specifically:
*   Predictions are clipped to the range [-6, 6] to handle outliers.
*   The correlation is weighted by the absolute value of the target (amplitude of price change).
*   Implementation details can be found in `utils.weighted_pearson_correlation`.

For each target *i* (where i is t0, t1), the score is calculated as the Weighted Pearson correlation between the true values and your predictions.

The final score is the average of the correlation scores across the two targets.

A higher correlation score is better!

## ðŸš€ Quick start

The fastest way to get started is to run the **Vanilla GRU baseline example** solution we've provided. This solution was generated by an LLM as a demonstration. This will help you understand the data flow and submission format.

```bash
# Navigate to the example directory
cd example_solution

# Run the baseline solution
python solution.py
```

For a full walkthrough, including setting up your Python environment, check out our detailed Quick Start Guide in `docs/quick_start.md`.

After running the example, you'll be ready to build your own model. The provided solution is just a basic placeholderâ€”the real fun is creating something more powerful!

> **Tip: What models could work?**
> Since this is a sequence modeling task, you could explore:
> *   Recurrent models like **LSTM** or **GRU**.
> *   Attention-based models like the **Transformer**.
> *   Newer architectures like **Mamba-2**.

## How to submit your solution

Your submission must be a `.zip` file containing a `solution.py` file.

### Required structure

Your `solution.py` must define a class named `PredictionModel`. This class must have a `predict` method with the following signature:

```python
import numpy as np
from utils import DataPoint

class PredictionModel:
    def __init__(self):
        # Initialize your model, internal state, etc.
        pass

    def predict(self, data_point: DataPoint) -> np.ndarray | None:
        # Your logic here.
        if not data_point.need_prediction:
            return None

        # When a prediction is needed, return a numpy array of length 2 (for t0 and t1).
        prediction = np.zeros(2) # Replace with your model's output
        return prediction
```

The `data_point` object passed to your `predict` method is an instance of the `DataPoint` class, which has the following attributes:
*   `seq_ix: int`: The ID for the current sequence.
*   `step_in_seq: int`: The step number within the sequence.
*   `need_prediction: bool`: Whether a prediction is required for this point.
*   `state: np.ndarray`: The current market state vector (containing p0..p11, v0..v11, dp0..1, dv0..1).

Your `predict` method should:
*   Return `None` when `need_prediction` is `False`.
*   Return a `numpy.ndarray` of shape `(2,)` when `need_prediction` is `True`.
*   Remember to manage and reset the model's state for each new sequence (`seq_ix`).

### Packaging your solution

Your solution might include more than just `solution.py` (e.g., model weight files, helper Python modules, configs). Make sure to include all necessary files in your zip archive.

You can create the zip archive from your solution directory with a command like this:

```bash
# From inside your solution folder (e.g., my_awesome_solution/)
# This command zips up everything in the current directory.
zip -r ../solution.zip .
```

> **Note:** Make sure `solution.py` is at the root level inside the zip archive, not inside another folder.

## What's in the box

We've provided the following files to help you:

*   `datasets/train.parquet`: A training dataset containing **10,721** sequences.
*   `datasets/valid.parquet`: A validation dataset containing **1,444** sequences. You can use this for local validation, but you are also free to create your own validation splits or strategies.
*   `utils.py`: Contains helper classes and the scoring function so you can check your performance locally.
*   `example_solution/solution.py`: A Vanilla GRU baseline solution generated by an LLM to show the required submission format.

Good luck, and have fun building! We can't wait to see what you create.
